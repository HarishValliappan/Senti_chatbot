{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load the Dataset\n",
    "df = pd.read_csv('../dataset/train.csv', encoding='latin1')\n",
    "df = df[['text', 'sentiment']].dropna()\n",
    "\n",
    "# 2. Preprocess the Data\n",
    "# Clean text data\n",
    "df['text'] = df['text'].str.lower().apply(lambda x: re.sub(r'[^a-z\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " text         0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df[['text', 'sentiment']]\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "le = LabelEncoder()\n",
    "df['sentiment'] = le.fit_transform(df['sentiment'])\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenize(text)\n",
    "\n",
    "# Tokenize the text in the dataframe\n",
    "tokenized_text = list(yield_tokens(df['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of tokens and count the frequency\n",
    "all_tokens = [token for sublist in tokenized_text for token in sublist]\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Create vocabulary with special tokens\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for idx, token in enumerate(token_counts.keys(), 2):\n",
    "    vocab[token] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "def text_to_sequence(text, vocab, max_length):\n",
    "    tokens = tokenize(text)\n",
    "    sequence = [vocab[token] for token in tokens]\n",
    "    return sequence[:max_length] + [vocab[\"<pad>\"]] * (max_length - len(sequence))\n",
    "\n",
    "df['sequence'] = df['text'].apply(lambda x: text_to_sequence(x, vocab, max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the vocabulary\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Vocabulary saved to vocab.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df['sequence'], df['sentiment'], test_size=0.2, random_state=32)\n",
    "train_dataset = SentimentDataset(list(X_train), list(y_train))\n",
    "val_dataset = SentimentDataset(list(X_val), list(y_val))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, dropout_rate=0.1, l2_lambda=0.001, num_classes=3):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(embedding_dim * max_length, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.flatten(x)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = nn.ReLU()(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the Model\n",
    "model = SentimentModel(vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "# 8. Train the Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for sequences, labels in loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 1.0866, Train Accuracy: 0.4005\n",
      "Validation Loss: 1.0816, Validation Accuracy: 0.4065\n",
      "Epoch 2/20\n",
      "Train Loss: 1.0823, Train Accuracy: 0.4038\n",
      "Validation Loss: 1.0848, Validation Accuracy: 0.4065\n",
      "Epoch 3/20\n",
      "Train Loss: 1.0745, Train Accuracy: 0.4178\n",
      "Validation Loss: 1.0718, Validation Accuracy: 0.4041\n",
      "Epoch 4/20\n",
      "Train Loss: 1.0655, Train Accuracy: 0.4254\n",
      "Validation Loss: 1.0667, Validation Accuracy: 0.4347\n",
      "Epoch 5/20\n",
      "Train Loss: 1.0615, Train Accuracy: 0.4327\n",
      "Validation Loss: 1.0539, Validation Accuracy: 0.4367\n",
      "Epoch 6/20\n",
      "Train Loss: 1.0416, Train Accuracy: 0.4535\n",
      "Validation Loss: 1.0182, Validation Accuracy: 0.4667\n",
      "Epoch 7/20\n",
      "Train Loss: 1.0237, Train Accuracy: 0.4672\n",
      "Validation Loss: 1.0250, Validation Accuracy: 0.4643\n",
      "Epoch 8/20\n",
      "Train Loss: 1.0320, Train Accuracy: 0.4590\n",
      "Validation Loss: 1.0276, Validation Accuracy: 0.4591\n",
      "Epoch 9/20\n",
      "Train Loss: 0.9969, Train Accuracy: 0.4927\n",
      "Validation Loss: 1.0379, Validation Accuracy: 0.4640\n",
      "Epoch 10/20\n",
      "Train Loss: 0.9698, Train Accuracy: 0.5133\n",
      "Validation Loss: 0.9989, Validation Accuracy: 0.4891\n",
      "Epoch 11/20\n",
      "Train Loss: 0.9272, Train Accuracy: 0.5496\n",
      "Validation Loss: 0.9848, Validation Accuracy: 0.5049\n",
      "Epoch 12/20\n",
      "Train Loss: 0.8749, Train Accuracy: 0.5860\n",
      "Validation Loss: 0.9987, Validation Accuracy: 0.5104\n",
      "Epoch 13/20\n",
      "Train Loss: 0.8229, Train Accuracy: 0.6210\n",
      "Validation Loss: 1.0053, Validation Accuracy: 0.5144\n",
      "Epoch 14/20\n",
      "Train Loss: 0.7532, Train Accuracy: 0.6612\n",
      "Validation Loss: 1.0600, Validation Accuracy: 0.5144\n",
      "Epoch 15/20\n",
      "Train Loss: 0.6609, Train Accuracy: 0.7135\n",
      "Validation Loss: 1.0559, Validation Accuracy: 0.5295\n",
      "Epoch 16/20\n",
      "Train Loss: 0.5564, Train Accuracy: 0.7686\n",
      "Validation Loss: 1.1223, Validation Accuracy: 0.5278\n",
      "Epoch 17/20\n",
      "Train Loss: 0.4568, Train Accuracy: 0.8165\n",
      "Validation Loss: 1.3015, Validation Accuracy: 0.5258\n",
      "Epoch 18/20\n",
      "Train Loss: 0.3579, Train Accuracy: 0.8599\n",
      "Validation Loss: 1.4210, Validation Accuracy: 0.5475\n",
      "Epoch 19/20\n",
      "Train Loss: 0.2922, Train Accuracy: 0.8931\n",
      "Validation Loss: 1.3812, Validation Accuracy: 0.5431\n",
      "Epoch 20/20\n",
      "Train Loss: 0.2332, Train Accuracy: 0.9141\n",
      "Validation Loss: 1.6189, Validation Accuracy: 0.5491\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'sentiment_model.pth'\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'sentiment_model.pth')\n",
    "print(\"Model saved as 'sentiment_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_langchain1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
