{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\haris\\miniconda3\\envs\\env_langchain1\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Downloading groq-0.13.0-py3-none-any.whl (108 kB)\n",
      "   ---------------------------------------- 0.0/108.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 108.8/108.8 kB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"Use personal API\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-latency Large Language Models (LLMs) are a crucial development in the field of Natural Language Processing (NLP) and have significant implications for various applications. Here are some of the key reasons why low-latency LLMs are important:\n",
      "\n",
      "1. **Real-time conversational AI**: Low-latency LLMs enable real-time conversational AI experiences, allowing for smooth and seamless interactions between humans and machines. This is particularly important for applications like customer service chatbots, voice assistants, and language translation systems.\n",
      "2. **Improved user experience**: By responding quickly to user input, low-latency LLMs provide a more responsive and engaging experience, which is essential for applications like gaming, virtual reality, and augmented reality.\n",
      "3. **Increased throughput**: Low-latency LLMs can process and respond to a larger volume of requests in a shorter amount of time, making them ideal for high-traffic applications like online chat platforms, social media, and e-commerce websites.\n",
      "4. **Enhanced decision-making**: Low-latency LLMs can provide more accurate and timely insights, enabling faster decision-making in applications like financial analysis, market research, and healthcare.\n",
      "5. **Advancements in research and development**: Low-latency LLMs enable researchers to explore new areas of NLP research, such as multi-task learning, meta-learning, and transfer learning, which can lead to breakthroughs in other areas of AI development.\n",
      "6. **Edge computing and IoT integration**: Low-latency LLMs can be deployed on edge devices, enabling real-time processing and analysis of data from IoT devices, thus creating new opportunities for applications like predictive maintenance, autonomous vehicles, and smart homes.\n",
      "7. **Cybersecurity and fraud detection**: Low-latency LLMs can be used to detect and prevent cyberattacks and fraudulent activities in real-time, reducing the risk of financial losses and protecting sensitive information.\n",
      "8. **Enhanced accessibility**: Low-latency LLMs can provide real-time accessibility features for individuals with disabilities, such as speech-to-text and text-to-speech systems, enabling greater independence and autonomy.\n",
      "9. **Content generation and creation**: Low-latency LLMs can generate content, such as text, images, and videos, in real-time, revolutionizing content creation and enabling new forms of creative expression.\n",
      "10. **Underpinning future AI advancements**: Low-latency LLMs will likely play a crucial role in the development of future AI applications, such as explainable AI, cognitive architectures, and human-AI collaboration, which will require fast and accurate processing of natural language data.\n",
      "\n",
      "In summary, low-latency LLMs have far-reaching implications for various applications and will likely drive innovation in NLP, AI, and other fields.\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a chat completion with the Llama model\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain the importance of low latency LLMs\"}\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\"\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_langchain1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
